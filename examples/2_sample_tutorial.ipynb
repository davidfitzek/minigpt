{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model Text Generation Demo\n",
    "\n",
    "This notebook demonstrates how to load a trained GPT model checkpoint and generate text from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from minigpt.model.transformer import GPTModel\n",
    "from minigpt.config.config import DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Model\n",
    "\n",
    "First, we'll load the model from a saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model checkpoint path\n",
    "checkpoint_path = \"checkpoints/model_epoch_1.pth\"\n",
    "\n",
    "# Initialize the model with default configuration\n",
    "config = DEFAULT_CONFIG.model\n",
    "model = GPTModel(config)\n",
    "\n",
    "# Load the weights\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Generation Function\n",
    "\n",
    "Let's define a function to generate text from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=100, temperature=0.8, top_k=40):\n",
    "    \"\"\"\n",
    "    Generate text from the model given a prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to start generation\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate\n",
    "        temperature (float): Sampling temperature (higher = more random)\n",
    "        top_k (int): Top-k sampling parameter (0 to disable)\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text including the prompt\n",
    "    \"\"\"\n",
    "    # Encode the prompt\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "\n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(output_ids[0].tolist())\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Text Samples\n",
    "\n",
    "Now let's generate some text with different prompts and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Once upon a time'\n",
      "Generated text:\n",
      "================================================================================\n",
      "Once upon a time, then the best of the most common will be more interested.\n",
      "\n",
      "When you are trying to see me all to keep that?\n",
      "\n",
      "And you are in the world of that in the world.\n",
      "\n",
      "I'm a good of his own person in one of my family.\n",
      "\n",
      "This is very a really one of you who is so many others.\n",
      "\n",
      "And you know?\n",
      "\n",
      "I believe them, don't be talking that?\n",
      "\n",
      "I believe it!\n",
      "\n",
      "I\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic generation with default parameters\n",
    "prompt = \"Once upon a time\"\n",
    "generated_text = generate_text(prompt)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"Generated text:\")\n",
    "print(\"=\" * 80)\n",
    "print(generated_text)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The future of artificial intelligence'\n",
      "Generated text:\n",
      "================================================================================\n",
      "The future of artificial intelligence. In the present hand, the majority as the first section of the world. The results in the case of the\n",
      "point of the form of the two-temperature are obtained by the two-dalo and a part of the\n",
      "direction of the current-interverse and the inner.\n",
      "\n",
      "\\begin{remark}[t]\n",
      "\\centering\n",
      "\\begin{tabular}[t]\n",
      "\\includegraphics[width=1.6\\columnwidth]{figures/0.01cm}\n",
      "\\caption{An=0.50\\tiny){9ures/0.5.}\n",
      "\\end{figure}\n",
      "\n",
      "\\caption{The following of the most-triv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Try with a different prompt\n",
    "prompt = \"The future of artificial intelligence\"\n",
    "generated_text = generate_text(prompt, max_new_tokens=150)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"Generated text:\")\n",
    "print(\"=\" * 80)\n",
    "print(generated_text)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'In a world where'\n",
      "Generated text (with temperature=1.0, top_k=100):\n",
      "================================================================================\n",
      "In a world where you have all been doing the right page used by you there for their computer for something for and you know of that they take it in you. I thought, so's help. Thank it in the way and put everything. You can make for a business and take you the right option in our face from the process or the first? We just come at the room.\n",
      "The process for their car: but you have better to learn our family about you. It is fun and it is great and very for the price, no a few-stop company. If it depends on the form this kind of your own, you can be not the only more that we will put a better.\n",
      "I don't definitely have a very important store that is as good unless you have to take a few of an hour. Like that you have a bit or to do like me see the entire time and get to help it out the amount.\n",
      "O/4-4/22 -\n",
      "\n",
      "I can\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Try with different sampling parameters\n",
    "prompt = \"In a world where\"\n",
    "generated_text = generate_text(\n",
    "    prompt,\n",
    "    max_new_tokens=200,\n",
    "    temperature=1.0,  # Higher temperature for more randomness\n",
    "    top_k=100,  # More candidates in top-k sampling\n",
    ")\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"Generated text (with temperature=1.0, top_k=100):\")\n",
    "print(\"=\" * 80)\n",
    "print(generated_text)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmark Generation Speed\n",
    "\n",
    "Let's measure how fast our model can generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking generation speed...\n",
      "Average generation time for 50 tokens: 2.623 seconds\n",
      "Tokens per second: 19.06\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def benchmark_generation(prompt, num_runs=5, tokens_per_run=100):\n",
    "    times = []\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        _ = generate_text(prompt, max_new_tokens=tokens_per_run)\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "    avg_time = np.mean(times)\n",
    "    tokens_per_second = tokens_per_run / avg_time\n",
    "\n",
    "    print(\n",
    "        f\"Average generation time for {tokens_per_run} tokens: {avg_time:.3f} seconds\"\n",
    "    )\n",
    "    print(f\"Tokens per second: {tokens_per_second:.2f}\")\n",
    "\n",
    "\n",
    "# Run benchmark\n",
    "print(\"Benchmarking generation speed...\")\n",
    "benchmark_generation(\"The quick brown fox\", num_runs=3, tokens_per_run=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
